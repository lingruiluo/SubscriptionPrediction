---
title: "Subscription Prediction"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Data Explanation

This dataset has been dowloaded from UCI Machine Learning Repository. "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed." (See https://archive.ics.uci.edu/ml/datasets/bank+marketing ). See also the `dataset_description.txt` and `attribute_info.txt` for explanation of the datasets and variables.

# Data Preparation

```{r warning=FALSE, message=FALSE}
# libraries
library(dplyr)
library(naniar)
library(visdat)
library(mice)
library(ggplot2)
library(corrplot)
library(e1071)
library(plotly)
library(ggmosaic)
```

```{r}
# read the data
data = read.csv("bank-additional/bank-additional-full.csv", sep = ";")
```

Take a look at the data:

```{r}
summary(data)
```

```{r}
str(data)
```

There are 21 variables and 41,188 observations in total. The types of our features are diverse.  
- numeric variables: `age`, `duration`, `campaign`, `pdays`(discrete), `previous`(discrete), `emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m`, `nr.employed`  
- categorical variables: `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week`, `poutcome`
- target variables: `y`

There are some variables with level **unknown**, which should be treated as missing values.  
Also, from the document `attribute_info.txt`, `pdays` (number of days that passed by after the client was last contacted from a previous campaign) has a value of 999 which means that client was not previously contacted. This variable is expected to be dealt with later regarding to the value of 999. 


## Missing value

```{r}
# number of missing values
cat("The number of missing values: ",sum(is.na(data)))
```

There is no "direct" missing value in the dataset. However, as we mentioned above, the entries marked as **unknown** should be treated as missing values. We replace all entries with **unknown** as NA.

```{r echo=FALSE}
data[data == "unknown"] = NA
```


After replacing all **unknown** values, let's plot the missing values.

```{r}
vis_miss(data, sort_miss = TRUE)
```

There are a few missing values associated with `job`, `marital`, `education`, `housing`, `loan`. But there are 20.87% missing values for `default`.

We use the `mice` package to impute the missing values.

```{r, eval = FALSE, echo=TRUE}
#imputed_data = mice(data, maxit = 1, method = 'pmm', seed = 6)
#data = complete(imputed_data)

# save the data
#write.csv(data, "cleaned_data_additional_full.csv")
```

```{r echo=TRUE}
# Now we can read the data directly
data = read.csv("cleaned_data_additional_full.csv")[,-1]
```

## Balance of Dataset 

In classification problem, the balance of dataset refers to the ratio of the number of output classes. Usually, 10:1 will be acceptable.  

```{r}
table(data$y)
cat("The ratio of balance: ", table(data$y)['no']/table(data$y)['yes'])
```

The dataset is balanced as the ratio is approximately 8:1.  

## Variable Exploration

### age

How is `age` distributed and how is it related to `y`?

```{r}
# distribution of variable age
ggplot(data, aes(x = age, color = y)) + 
  geom_histogram(fill = "white", alpha=0.8, position="identity") +
  ggtitle("The Distribution of Age") +
  xlab("Age") + 
  ylab("Count") +
  scale_color_brewer(palette = "Paired")
```

We can see that most of the potential subscribers are middle-aged. It also seems that even though most potential subscribers are middle-aged, the proportion of these people who actually subscribed are comparably low. The campaign works well in young and old people, but proportionally we do not have a lot of such customers.

### Job

How is `job` related to `y`?

```{r}
data %>% 
  group_by(y) %>%
  count(job) %>%
  ungroup() %>%
#ggplot(data, aes(fill=y, y=..count.., x=job)) + 
  #geom_bar(position="stack", stat="count") + 
  ggplot(aes(x=reorder(job,n),y=n,fill=y))+
  geom_bar(stat="identity")+
  ggtitle("How many observations for each type of jobs?") +
  xlab("Jobs") +
  ylab("Count") +
  coord_flip() +
  scale_fill_brewer(palette = "Paired")
```

Based on the plot, the proportion of retired and student subscribers are optimal. Most of people in our dataset are admin., blue-collar and technician, however, the proportion of subscribers is just a few. 

### marital status

```{r}
ggplot(data, aes(fill=y, y=..count.., x=marital)) + 
  geom_bar(position="stack", stat="count") +
  ggtitle("How many observations for each marital status?") +
  xlab("Marital Status") +
  ylab("Count") +
  scale_fill_brewer(palette = "Paired")
```

There are many married people but just a few of them are subscribers. 

### education

```{r}
data %>% 
  group_by(y) %>%
  count(education) %>%
  ungroup() %>%
  ggplot(aes(x=reorder(education,n),y=n,fill=y))+
    geom_bar(stat="identity") +
    ggtitle("How many observations for each type of education?") +
    xlab("Education") +
    ylab("Count") +
    coord_flip() +
    scale_fill_brewer(palette = "Paired")
#ggplot(data, aes(fill=y, y=..count.., x=education)) + 
#    geom_bar(position="stack", stat="count") +
```

Most people are educated, and the majority obtained university degree. It is hard to say that higher-educated people are more likely to subscribe. 

### default

```{r}
data %>% 
  group_by(y) %>%
  count(default) %>%
  ungroup() %>%
  #ggplot(data, aes(fill=y, y=..count.., x=default)) + 
  #  geom_bar(position="stack", stat="count")
  ggplot(aes(x=reorder(default,-n),y=n,fill=y))+
    geom_bar(stat="identity") +
    ggtitle("How many observations having credit in default?") +
    xlab("Default") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

Looks like only few observations have `default` = yes. We look at these observations.

```{r}
# observations where default = yes
data$y[data$default == "yes"]
```

Most of the observations did not subscribe. But we cannot say there is any pattern, since this is the case within the whole population.

### contact

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=contact)) + 
#    geom_bar(position="stack", stat="count")
data %>% 
  group_by(y) %>%
  count(contact) %>%
  ungroup() %>%
  ggplot(aes(x=reorder(contact,-n),y=n,fill=y))+
    geom_bar(stat="identity") +
    ggtitle("How many observations for each type of contact?") +
    xlab("Contact") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

The plot shows that eople who are contacted by celluar phone are more likely to subscribe.

### housing

```{r}
ggplot(data = data) +
   geom_mosaic(aes(x = product(y, housing), fill = housing), na.rm=TRUE) + 
  labs(x = "loan ", y = 'Subscribed', title='How Is Housing Related to the Results?') +
  scale_fill_brewer(palette = "Paired")
```

The `housing` variable has only little affect on the results.

### loan

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=loan)) + 
#    geom_bar(position="stack", stat="count")
ggplot(data = data) +
   geom_mosaic(aes(x = product(y, loan), fill = loan), na.rm=TRUE) + 
  labs(x = "loan ", y = 'Subscribed', title='How Is Loan Related to the Results?') +
  scale_fill_brewer(palette = "Paired")
```

The `loan` variable has only little affect on the results.

### month

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=month)) + 
#    geom_bar(position="stack", stat="count")
data %>% 
  group_by(y) %>%
  count(month) %>% 
  ggplot(aes(x=reorder(month,-n), y=n, fill=y)) +
    geom_bar(stat="identity") +
    ggtitle("How many observations for each month?") +
    xlab("Month") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

From the plot, we can see that there are many more observation in May. However, the proportion of people subscribed are not optimal. Also, there are not many observations on December, March, October, September, but the proportion of subscriptions is pretty high.

### day_of_week

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=day_of_week)) + 
#    geom_bar(position="stack", stat="count")
data %>% 
  group_by(y) %>%
  count(day_of_week) %>% 
  ggplot(aes(x=reorder(day_of_week,-n), y=n, fill=y)) +
    geom_bar(stat="identity") +
    ggtitle("How many observations for each last contact day of the week?") +
    xlab("Last contact day of the week") +
    ylab("Count") +
    scale_fill_brewer(palette = "Paired")
```

There aren't siginificant differences in `day_of_week`.

### duration

```{r, warning = FALSE, message=FALSE}
#ggplot(data, aes(fill=y, y=..count.., x=duration)) + 
#    geom_histogram()
ggplot(data, aes(x = duration, color = y)) + 
  geom_histogram(fill = "white", alpha=0.8, position="identity") +
  ggtitle("The Distribution of duration") +
  xlab("Duration") + 
  ylab("Count") +
  scale_color_brewer(palette = "Paired")
```

When duration is near 0, almost no one subscribed. However, as duration increases, it seems that the proportion of people who subscribed increases.

### campaign

```{r}
ggplot(data, aes(fill=y, y=..count.., x=campaign)) + 
  geom_bar(position="stack", stat="count") +
  scale_fill_brewer(palette = "Paired") +
  ggtitle("How many number of contacts performed during this campaign and for this client?") +
  xlab("Campaign") +
  ylab("Count")
```

### pdays

```{r}
# may be change level 999?
ggplot(data, aes(fill=y, y=..count.., x=pdays)) + 
  geom_bar(width = 100, position="stack", stat="count") +
  scale_fill_brewer(palette = "Paired") +
  ggtitle("How many number of days that passed by \nafter the client was last contacted from a previous campaign? \n (999 means client was not previously contacted)") +
  xlab("Number of Days") +
  ylab("Count")
```

Most of clients not previouly being contacted. Therefore, we plot again except the value of 999.  

```{r}
data %>% filter(pdays!=999) %>%
  ggplot(aes(fill=y, y=..count.., x=pdays)) + 
    geom_bar(position="stack", stat="count") +
    scale_fill_brewer(palette = "Paired") +
    ggtitle("How many number of days that passed by \nafter the client was last contacted from a previous campaign?") +
    xlab("Number of Days") +
    ylab("Count") +
    labs(caption="Note: The values of 999 have been discarded in this plot.")
```

Most of clients who were previously contacted were contacted less than 10 days ago. 

### previous

```{r, warning = FALSE}
ggplot(data, aes(fill=y, y=..count.., x=previous)) + 
    geom_bar(position="stack", stat="count") +
    scale_fill_brewer(palette = "Paired") +
    ggtitle("How many number of contacts performed before this campaign and for this client?") +
    xlab("Number of contacts") +
    ylab("Count")
```

### poutcome

```{r}
ggplot(data, aes(fill=y, y=..count.., x=poutcome)) + 
    geom_bar(position="stack", stat="count") +
    scale_fill_brewer(palette = "Paired") +
    ggtitle("Outcome of the previous marketing campaign") +
    xlab("Previous marketing campaign") +
    ylab("Count")
```

From the plot, we found that if previous marketing campagin succeeded, people are more likely to subscribe.

### emp.var.rate

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=emp.var.rate)) + 
#  geom_histogram(bins=6, position='stack') +
ggplot(data, aes(x=y,y=emp.var.rate,color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Employment variation rate (quarterly indicator)") + 
  ylab("Employment variation rate")
```

### cons.price.idx

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=cons.price.idx)) + 
#    geom_histogram(bins=5)
ggplot(data, aes(x=y,y=cons.price.idx,color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Consumer price index (monthly indicator)") + 
  ylab("Consumer price index")
```

### cons.conf.idx

```{r}
ggplot(data, aes(x=y,y=cons.conf.idx, color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Consumer Confidence Index (monthly indicator)") + 
  ylab("Consumer confidence index")

```

### euribor3m

```{r}
ggplot(data, aes(x=y,y=euribor3m, color=y)) +
  geom_boxplot() +
  scale_color_brewer(palette = "Paired") +
  ggtitle("The distribution of Euribor 3 month rate (daily indicator)") + 
  ylab("Euribor 3 month rate")
```

### nr.employed

```{r}
#ggplot(data, aes(fill=y, y=..count.., x=nr.employed)) + 
#    geom_histogram(bins=4)

# distribution of variable age
ggplot(data, aes(x = nr.employed, color = y)) + 
  geom_histogram(fill="white",alpha=0.8, position="identity", bins=4) +
  ggtitle("The distribution of number of employees (quarterly indicator)") +
  xlab("Number of Employees") + 
  ylab("Count") +
  scale_color_brewer(palette = "Paired")
```

# Feature Engineering

## Correlation

We check the correlations of the variables.

```{r}
X = data %>%
  select(-y)

# change into integer
for(i in 1:ncol(X)){
  X[,i] <- as.integer(X[,i])
}


corr_mat=cor(X,method="s")
corrplot(corr_mat, is.corr = FALSE, win.asp = .7, method = "circle")
```

We denote that `emp.var.rate`, `euribor3m` and `nr.employed` are highly correlated to each other.

We look at the description of the three variables:

1. emp.var.rate: employment variation rate - quarterly indicator (numeric)

2. euribor3m: euribor 3 month rate - daily indicator (numeric)

3. nr.employed: number of employees - quarterly indicator (numeric)

We may consider remove `emp.var.rate` and `euribor3m` while modelling.

Also, we remove the `duration` variable based on the dataset author's note: "this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model."

```{r}
data = data %>%
  select(-c(emp.var.rate, euribor3m, duration))
```

# Modeling  

```{r echo=FALSE, message=FALSE}
library(caret)
library(catboost)
library(pROC)
library(ROCR)
library(xgboost)
library(cattonum)
library(mltools)
library(randomForest)
library(naivebayes)
```


```{r}
# train-test split
set.seed(2383)
size = floor(0.75 * nrow(data))
train_index <- sample(seq_len(nrow(data)), size = size)
train = data[train_index,]
train_X = train %>% select(-y)
test = data[-train_index,]
test_X = test %>% select(-y)
```


## Logistic Regression

We first fit a logistic regression model since the response variable is binary.

```{r}
train_logit <- train[,]
test_logit <- test[,]
levels(train_logit$y) <- c(0,1)
levels(test_logit$y) <- c(0,1)
```

```{r, cache = TRUE}
logit_mod <- glm(y ~ ., data = train, family = binomial)
```

Since there are much less observations who subscribed to a term deposit, we define a threshold that if the predicted value is greater than 0.5, we treat it as subscribed.

```{r, message = FALSE}
logit_pred = logit_mod %>% predict(test_X , type = "response")
logit_pred = ifelse(logit_pred > 0.5, 1, 0)
logit_pred_acc = mean(logit_pred == test_logit$y)
cat("Prediction Accuracy Rate: ",logit_pred_acc)
```

The accuracy achieved is `r logit_pred_acc` To further invest on this model, we look at the confusion matrix.

```{r}
table(test_logit$y, logit_pred > 0.5)
```

```{r message=FALSE}
logit_roc <- roc(test_logit$y,logit_pred,direction="<")
logit_auc <- auc(logit_roc)
plot(logit_roc, main=paste("ROC Curve (AUC Score: ",logit_auc,")"))
```

We found that about `r 128/(8988+128)`% people who did not subscribe but are predicted as subscribed. About `r 885/(885+296)`% people who subscribed but are predicted as not subscribed. We would like to try with a more rigorous threshold to reduce the true negative rates. After trying several thresholds, we found that with threshold = 0.1, the false positive rate is the lowest. However, it classifies too many people who did not subscribe into subscribed, which could result in too much effort in reaching out people who are supposed to subscribe. Therefore, we decide to use a threshold of 0.2, which makes balances the false positive rate the the true negative rate.

```{r}
logit_pred = predict(logit_mod, test_X, type = "response")
logit_pred = ifelse(logit_pred > 0.2, 1, 0)
logit_pred_acc = mean(logit_pred == test_logit$y)
cat("Prediction Accuracy Rate: ",logit_pred_acc)
```

```{r}
table(test_logit$y, logit_pred > 0.2)
```


```{r message=FALSE}
logit_roc <- roc(test_logit$y,logit_pred,direction="<")
logit_auc <- auc(logit_roc)
plot(logit_roc, main=paste("ROC Curve (AUC Score: ",logit_auc,")"))
```

The overall accuracy decreases to `r logit_pred_acc`, since it classifies more people who did not subscribe into subscribe. However, the false positive rate actually declines to `r 855/(855+8261)`%, the goal to target potential customers are better achieved without contacting too many clients.

The model works well in identifying potential customers. We will try more advanced models next.


## Naive Bayes 


```{r message=FALSE, echo=FALSE}
nb_control <- trainControl(
  method = "cv",
  number = 10,
  verbose = FALSE
)
nb_grid <- expand.grid(
  usekernel = TRUE,
  fL = 1,
  adjust = 5
)
nb_model <- train(
  x = train %>% select(-y),
  y = train$y,
  method = "nb",
  trControl = nb_control,
  tuneGrid = nb_grid,
  preProc = c("center","scale")
)
```

```{r message=FALSE, echo=FALSE}
nb_pred = predict(nb_model, test)
nb_pred_acc = sum(nb_pred == test$y) / length(test$y)
```

```{r message=FALSE}
nb_roc <- roc(as.integer(test$y),as.integer(nb_pred),direction="<")
nb_auc <- auc(nb_roc)
plot(nb_roc, main=paste("ROC Curve (AUC Score: ",nb_auc,")"))
```

The accuracy rate is `r nb_pred_acc`, and the auc score is `r nb_auc`.  

## SVM

It takes too long to train. 

```
svm_linear_control <- trainControl(
  method = "cv",
  number = 5,
  verbose = FALSE
)

svm_linear_grid <- expand.grid(
  C = c(0.1, 1, 10)
)

svm_linear_model <- train(
  y~.,
  data = train,
  method = "svmLinear",
  trControl = svm_linear_control,
  tuneGrid = svm_linear_grid,
  preProc = c("scale","center"),
  verbose = FALSE
)
svm_linear_model$bestTune
```

```
# radiao kernel 
```


## Random Forest

```{r, cache = TRUE}
rf_grid = expand.grid(mtry = 1 : 8)

## out of bag
oob = trainControl(method = "oob")
rf_oob_tune = train(
  y ~ .,
  data = train,
  method = "rf",
  trControl = oob,
  verbose = FALSE,
  tuneGrid = rf_grid
  )

rf_oob_tune$bestTune
```

```{r}
varImp(rf_oob_tune, scale = FALSE)
```

```{r}
rf_oob_pred = predict(rf_oob_tune, test)
rf_oob_pred_acc = sum(rf_oob_pred == test$y) / length(test$y)
cat("Prediction Accuracy Rate: ",rf_oob_pred_acc)
```

The best tune uses `mtry` = 8. By checking the variable importance, we can see that `duration`, `euribor3m`, and `age` are the most important variables for predicting whether a client subscribes to a term deposit or not. The prediction accuracy is **`r rf_oob_pred_acc`** on the test data.  

```{r message=FALSE}
rf_roc <- roc(as.integer(test$y),as.integer(rf_oob_pred),direction="<")
rf_auc <- auc(rf_roc)
plot(rf_roc, main=paste("ROC Curve (AUC Score: ",rf_auc,")"))
```

From the AUC-ROC curve, the random forest model is good but is not as good as logistic regression.

## Gradient Boosting

```{r}
gb_control <- trainControl(
   method = "cv",
   number = 5
)

gb_grid <-  expand.grid(interaction.depth = c(3, 5, 9), 
                        n.trees = 500,
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

gb_model <- train(y ~ ., data = train, 
                 method = "gbm", 
                 trControl = gb_control, 
                 verbose = FALSE, 
                 tuneGrid = gb_grid)
```

```{r}
gb_pred <- predict(gb_model, test)
gb_pred_acc <- sum(gb_pred == test$y) / length(test$y)
cat("Prediction Accuracy Rate: ",gb_pred_acc)
```

```{r message=FALSE}
gb_roc <- roc(as.integer(test$y),as.integer(gb_pred),direction="<")
gb_auc <- auc(gb_roc)
plot(gb_roc, main=paste("ROC Curve (AUC Score: ",gb_auc,")"))
```

The accuracy rate is `r gb_pred_acc` and the auc score is `r gb_auc`. 

## XGBoost 


```{r}
xgb_control <- trainControl(
  method = "cv",
  number = 5,
  search = "random",
  savePredictions = TRUE,
  classProbs = TRUE
)

xgb_grid <- expand.grid(
  eta=0.1,
  gamma=1,
  max_depth=6,
  colsample_bytree=0.9,
  nrounds=10,
  min_child_weight=10,
  subsample = 0.9
  )

xgb_model <- train(
  y ~.,
  data = train,
  method = "xgbTree",
  metric = "Accuracy",
  nthread = 2,
  verbosity = 0,
  tuneGrid = xgb_grid,
  trControl = xgb_control
)
xgb_model$bestTune
```

```{r}
xgb_pred <- predict(xgb_model, test)
xgb_pred_acc <- sum(xgb_pred == test$y) / length(test$y)
cat("Prediction Accuracy Rate: ",xgb_pred_acc)
```

```{r}
varImp(xgb_model,scale = FALSE)
```

```{r message=FALSE}
xgb_roc <- roc(as.integer(test$y),as.integer(xgb_pred),direction="<")
xgb_auc <- auc(xgb_roc)
plot(xgb_roc, main=paste("ROC Curve (AUC Score: ",xgb_auc,")"))
```

The accuracy rate is `r xgb_pred_acc` and the auc score is `r xgb_auc`. 

## CatBoost  

We notice that there are many categorical variables, and also some discrete variables which can also be treaded as categorical. CatBoost is a powerful model which performance on dataset with many categorical variables in that it has various methods to handle categorical variables. However, since some of categorical variables seem to be high cardinal, the result may not be as good as we expect.  

```{r}
# extract features and labels for modeling
features <- train %>% select(-y)
labels <- train$y
```

```{r message=FALSE}
cat_control <- trainControl(
  method = "cv",
  number = 5,
  search = "random",
  classProbs = TRUE
)
cat_grid <- expand.grid(
  depth = c(4,6,8),
  learning_rate = c(0.01, 0.1, 1),
  l2_leaf_reg = c(0.1,1,2,4,6),
  rsm = 0.95,
  border_count = 64,
  iterations = 10
  
)
cat_model <- train(
  x = features,
  y = labels,
  method = catboost.caret,
  metric = "Accuracy",
  bootstrap_type = 'Bayesian',
  maximize = TRUE,
  logging_level='Silent',
  tuneGrid = cat_grid,
  trControl = cat_control
)
cat_model$bestTune
```

```{r}
cat_pred <- predict(cat_model, test)
cat_pred_acc <- sum(cat_pred == test$y) / length(test$y)
cat("Prediction Accuracy Rate: ",cat_pred_acc)
```

```{r}
varImp(cat_model,scale = FALSE)
```

```{r, message=FALSE}
cat_roc <- roc(as.integer(test$y),as.integer(cat_pred),direction="<")
cat_auc <- auc(cat_roc)
plot(cat_roc, main=paste("ROC Curve (AUC Score: ",cat_auc,")"))
```

The prediction accuracy rate on our test dataset is **`r cat_pred_acc`**, and the AUC score is `r cat_auc`. Our best parameters selected from the grid search is also shown above. From our best model, `nr.employed` is the most important variable. 

# Results  

